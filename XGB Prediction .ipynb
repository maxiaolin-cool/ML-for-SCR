{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d0b26a-15b2-4a6e-875b-4cab30d0d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤–å±‚æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°...\n",
      "\n",
      "å¤–å±‚æ¨¡å‹å·²ä¿å­˜è‡³ï¼šxgboost_model_p\\xgboost_model.pkl\n",
      "\n",
      "å¤–å±‚æ¨¡å‹æ€§èƒ½ï¼š\n",
      "       Train R2   Test R2  Train RMSE  Test RMSE\n",
      "Value  0.999906  0.798274     0.00093   0.053508\n",
      "\n",
      "å¼€å§‹å†…å±‚5æŠ˜äº¤å‰éªŒè¯...\n",
      "\n",
      "æ­£åœ¨å¤„ç†ç¬¬ 1 æŠ˜...\n",
      "ç¬¬ 1 æŠ˜å®Œæˆ - éªŒè¯é›†R2: 0.6247\n",
      "\n",
      "æ­£åœ¨å¤„ç†ç¬¬ 2 æŠ˜...\n",
      "ç¬¬ 2 æŠ˜å®Œæˆ - éªŒè¯é›†R2: 0.2189\n",
      "\n",
      "æ­£åœ¨å¤„ç†ç¬¬ 3 æŠ˜...\n",
      "ç¬¬ 3 æŠ˜å®Œæˆ - éªŒè¯é›†R2: 0.3950\n",
      "\n",
      "æ­£åœ¨å¤„ç†ç¬¬ 4 æŠ˜...\n",
      "ç¬¬ 4 æŠ˜å®Œæˆ - éªŒè¯é›†R2: 0.1628\n",
      "\n",
      "æ­£åœ¨å¤„ç†ç¬¬ 5 æŠ˜...\n",
      "ç¬¬ 5 æŠ˜å®Œæˆ - éªŒè¯é›†R2: 0.1160\n",
      "\n",
      "äº¤å‰éªŒè¯ç»“æœæ±‡æ€»ï¼š\n",
      "   Train R2   Test R2  Train RMSE  Test RMSE\n",
      "0  0.999811  0.624740    0.001389   0.044537\n",
      "1  0.987335  0.218892    0.010806   0.081226\n",
      "2  0.993186  0.394989    0.008324   0.056880\n",
      "3  0.993064  0.162828    0.006520   0.131639\n",
      "4  0.975746  0.115967    0.015749   0.066475\n",
      "\n",
      "äº¤å‰éªŒè¯å¹³å‡æ€§èƒ½ï¼š\n",
      "Train R2: 0.9898 Â± 0.0090\n",
      "Test R2: 0.3035 Â± 0.2084\n",
      "Train RMSE: 0.0086 Â± 0.0053\n",
      "Test RMSE: 0.0762 Â± 0.0338\n",
      "\n",
      "å®é™…å€¼ vs é¢„æµ‹å€¼å›¾å·²ä¿å­˜è‡³ï¼šxgboost_model_p\\actual_vs_predicted.png\n",
      "\n",
      "è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³ results_summary.txt\n",
      "\n",
      "åµŒå¥—äº¤å‰éªŒè¯å®Œæˆï¼æ‰€æœ‰ç»“æœä¿å­˜åœ¨ç›®å½•ï¼š xgboost_model_p\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, mean_squared_error, r2_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ==================== å‚æ•°é…ç½® ====================\n",
    "input_path = \"é¢„æµ‹æ•°æ®å½’ä¸€åŒ–å-20251120.xlsx\"  # è¾“å…¥æ•°æ®è·¯å¾„\n",
    "output_dir = \"xgboost_model_p\"     # è¾“å‡ºç›®å½•\n",
    "target_column = \"SO2 tolerance\"     # ç›®æ ‡å˜é‡åˆ—å\n",
    "outer_test_size = 0.2               # å¤–å±‚æµ‹è¯•é›†æ¯”ä¾‹\n",
    "inner_test_size=0.2                 # å†…å±‚æµ‹è¯•é›†æ¯”ä¾‹   \n",
    "random_state = 42                   # éšæœºç§å­\n",
    "n_splits = 5                        # äº¤å‰éªŒè¯æŠ˜æ•°\n",
    "model_path = os.path.join(output_dir, \"xgboost_model.pkl\")  # æ¨¡å‹ä¿å­˜è·¯å¾„\n",
    "\n",
    "# ==================== å‡½æ•°å®šä¹‰ ====================\n",
    "def prepare_data(input_path, target_column):\n",
    "    \"\"\"æ•°æ®åŠ è½½ä¸é¢„å¤„ç†\"\"\"\n",
    "    df = pd.read_excel(input_path)\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"ç›®æ ‡åˆ— {target_column} ä¸å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "    \n",
    "    y = df[target_column]\n",
    "    X = df.drop(columns=[target_column])\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    return X[numeric_cols], y\n",
    "\n",
    "def determine_problem_type(y):\n",
    "    \"\"\"è‡ªåŠ¨åˆ¤æ–­é—®é¢˜ç±»å‹\"\"\"\n",
    "    if y.nunique() <= 10:  # åˆ†ç±»é—®é¢˜ï¼ˆç±»åˆ«æ•°â‰¤10ï¼‰\n",
    "        return ('binary:logistic' if y.nunique() == 2 else 'multi:softprob', \n",
    "                \"classification\")\n",
    "    return ('reg:squarederror', \"regression\")\n",
    "\n",
    "def get_model_params(objective, random_state):\n",
    "    \"\"\"è·å–æ¨¡å‹å‚æ•°\"\"\"\n",
    "    return {\n",
    "        'objective': objective,\n",
    "        'eval_metric': 'logloss' if objective.startswith('binary') else 'rmse',\n",
    "        'eta': 0.1,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'seed': random_state,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, params, problem_type):\n",
    "    \"\"\"è®­ç»ƒæ¨¡å‹å¹¶è¿”å›è¯„ä¼°ç»“æœ\"\"\"\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # è¯„ä¼°æ¨¡å‹\n",
    "    ddata_train = xgb.DMatrix(X_train)\n",
    "    ddata_test = xgb.DMatrix(X_test)\n",
    "    \n",
    "    if problem_type == \"classification\":\n",
    "        y_pred_train =np.round(model.predict(ddata_train)) if params['objective'] == 'binary:logistic' else np.argmax(model.predict(ddata_train), axis=1)\n",
    "        y_pred_test =np.round(model.predict(ddata_test)) if params['objective'] == 'binary:logistic' else np.argmax(model.predict(ddata_test), axis=1)\n",
    "        metrics = {\n",
    "            'Train Accuracy': accuracy_score(y_train, y_pred_train),\n",
    "            'Test Accuracy': accuracy_score(y_test, y_pred_test),\n",
    "            'Train F1': f1_score(y_train, y_pred_train, average='macro'),\n",
    "            'Test F1': f1_score(y_test, y_pred_test, average='macro')\n",
    "        }\n",
    "    else:\n",
    "        y_pred_train = model.predict(ddata_train)\n",
    "        y_pred_test = model.predict(ddata_test)\n",
    "        metrics = {\n",
    "            'Train R2': r2_score(y_train, y_pred_train),\n",
    "            'Test R2': r2_score(y_test, y_pred_test),\n",
    "            'Train RMSE': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "            'Test RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        }\n",
    "    return model, metrics\n",
    "def plot_feature_importance(model, output_dir):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é¢œè‰²æ–¹æ¡ˆï¼‰\n",
    "    \n",
    "    å‚æ•°:\n",
    "        model: è®­ç»ƒå¥½çš„XGBoostæ¨¡å‹\n",
    "        output_dir: è¾“å‡ºç›®å½•\n",
    "    \"\"\"\n",
    "    # è‡ªå®šä¹‰é¢œè‰²æ–¹æ¡ˆï¼ˆæ ¹æ®ç”¨æˆ·æä¾›çš„é¢œè‰²ï¼‰\n",
    "    custom_colors = {\n",
    "        'EN.': '#82BBF0', \n",
    "        'P.Tim.': '#DFF1FF', \n",
    "        'S.A.': '#FDB3CC', \n",
    "        'I.R': '#82BBF0', \n",
    "        'I.E.': '#82BBF0', \n",
    "        'P.D.': '#FDB3CC',\n",
    "        'H2O': '#FF78A8', \n",
    "        'P.Tem.': '#DFF1FF', \n",
    "        'M.P.': '#82BBF0', \n",
    "        'T.C.': '#82BBF0', \n",
    "        'P.V.': '#FDB3CC', \n",
    "        'C.Tem.': '#FDB3CC',\n",
    "        'Density': '#82BBF0',\n",
    "        'NH3': '#FF78A8', \n",
    "        'SO2': '#DFF1FF', \n",
    "        'C.Tim.': '#FDB3CC', \n",
    "        'O2': '#FF78A8', \n",
    "        'NO': '#FF78A8',\n",
    "        'GHSV': '#FF78A8'\n",
    "    }\n",
    "    \n",
    "    # è®¾ç½®å…¨å±€å­—ä½“æ ·å¼\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    \n",
    "    # è®¾ç½®å›¾å½¢å¤§å°å’Œè¾¹ç¼˜è·ç¦»\n",
    "    fig, ax = plt.subplots(figsize=(10, 12), facecolor='none')\n",
    "    plt.subplots_adjust(left=0.2, right=0.95, top=0.9, bottom=0.1)\n",
    "    ax.grid(False)\n",
    "    ax.set_facecolor('none')  # é€æ˜èƒŒæ™¯\n",
    "    \n",
    "    \n",
    "    # è·å–ç‰¹å¾é‡è¦æ€§æ•°æ®\n",
    "    importance = model.get_score(importance_type='weight')\n",
    "    importance = sorted(importance.items(), key=lambda x: x[1], reverse=False)\n",
    "    feature_names = [x[0] for x in importance]\n",
    "    feature_imp = [x[1] for x in importance]\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªç‰¹å¾åˆ†é…é¢œè‰²ï¼ˆå¦‚æœç‰¹å¾æœªåœ¨è‡ªå®šä¹‰é¢œè‰²ä¸­å®šä¹‰ï¼Œä½¿ç”¨é»˜è®¤é¢œè‰²ï¼‰\n",
    "    colors = [custom_colors.get(feature, '#FF78A8') for feature in feature_names]\n",
    "    \n",
    "    # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾\n",
    "    bars = ax.barh(\n",
    "        range(len(feature_names)), \n",
    "        feature_imp, \n",
    "        height=0.8,  # è°ƒæ•´æŸ±å½¢å®½åº¦\n",
    "        color=colors,  # ä½¿ç”¨è‡ªå®šä¹‰é¢œè‰²\n",
    "        \n",
    "        \n",
    "    )\n",
    "    \n",
    "    # è®¾ç½®yè½´åˆ»åº¦\n",
    "    ax.set_yticks(range(len(feature_names)))\n",
    "    ax.set_yticklabels(feature_names)\n",
    "    \n",
    "    # è®¾ç½®xè½´æ ‡ç­¾\n",
    "    plt.xlabel(\n",
    "        \"Importance\",\n",
    "        fontdict={\n",
    "            'fontname': 'Arial',\n",
    "            'fontsize': 24,\n",
    "            'fontweight': 'bold'\n",
    "        },\n",
    "        labelpad=10  # æ ‡ç­¾ä¸è½´çš„è·ç¦»\n",
    "    )\n",
    "    \n",
    "    # è®¾ç½®yè½´æ ‡ç­¾\n",
    "    plt.ylabel(\n",
    "        ' ',\n",
    "        fontdict={\n",
    "            'fontname': 'Arial',\n",
    "            'fontsize': 20,\n",
    "            'fontweight': 'bold'\n",
    "        },\n",
    "        labelpad=10  # æ ‡ç­¾ä¸è½´çš„è·ç¦»\n",
    "    )\n",
    "    \n",
    "    # è®¾ç½®åæ ‡è½´åˆ»åº¦å­—ä½“\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(20)\n",
    "    \n",
    "    # ä¿ç•™æ‰€æœ‰è¾¹æ¡†å¹¶è®¾ç½®æ ·å¼\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(2.5)  # åŠ ç²—è¾¹æ¡†çº¿\n",
    "        spine.set_color('black')  # è®¾ç½®è¾¹æ¡†é¢œè‰²\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(\n",
    "            width + max(feature_imp)*0.01,  # ä½ç½®å¾®è°ƒ\n",
    "            bar.get_y() + bar.get_height()/2,\n",
    "            f'{width:.1f}',\n",
    "            ha='left',\n",
    "            va='center',\n",
    "            fontname='Arial',\n",
    "            fontsize=18,\n",
    "            color='none' \n",
    "        )\n",
    "    \n",
    "    # ä¿å­˜å›¾åƒ\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, \"feature_importance.png\"), \n",
    "        dpi=300, \n",
    "        bbox_inches='tight',\n",
    "        transparent=False\n",
    "    )\n",
    "    plt.close()\n",
    "    \n",
    "def plot_actual_vs_predicted(X_train, y_train, X_test, y_test, model, output_dir):\n",
    "    \"\"\"ç»˜åˆ¶å®é™…å€¼ vs é¢„æµ‹å€¼å›¾ï¼ˆå›å½’é—®é¢˜ï¼‰\"\"\"\n",
    "    # è®¾ç½®å…¨å±€å­—ä½“æ ·å¼\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "        \n",
    "    d_train = xgb.DMatrix(X_train)\n",
    "    pp_tr = model.predict(d_train)\n",
    "    d_test = xgb.DMatrix(X_test)\n",
    "    y_predicted = model.predict(d_test)\n",
    "    \n",
    "    plt.figure(figsize=(10, 12))\n",
    "    g = sns.JointGrid(x=y_test, y=y_predicted, height=8, space=0)\n",
    "    \n",
    "    # ç»˜åˆ¶æ•£ç‚¹å›¾å’Œå›å½’çº¿\n",
    "    sns.scatterplot(x=y_train, y=pp_tr, s=100, color='#DB7987', ax=g.ax_joint, label='Training')\n",
    "    sns.scatterplot(x=y_test, y=y_predicted, s=100, color='#82BBF0', ax=g.ax_joint, label='Testing')\n",
    "    sns.regplot(x=y_test, y=y_predicted, ax=g.ax_joint, scatter=False, color='gray')\n",
    "       \n",
    "    # è°ƒæ•´åæ ‡è½´åˆ»åº¦æ ‡ç­¾æ ·å¼\n",
    "    for ax in [g.ax_joint, g.ax_marg_x, g.ax_marg_y]:\n",
    "        ax.tick_params(\n",
    "            axis='both', \n",
    "            which='major', \n",
    "            labelsize=20,      # åˆ»åº¦æ ‡ç­¾å¤§å°\n",
    "            colors='black',   # åˆ»åº¦æ ‡ç­¾é¢œè‰²\n",
    "            width=3,\n",
    "            length=8,\n",
    "            direction='out'\n",
    "\n",
    "        )\n",
    "    # è®¾ç½®å›¾ä¾‹æ ·å¼\n",
    "    g.ax_joint.legend(\n",
    "        title_fontsize=20,      # æ ‡é¢˜å­—ä½“å¤§å°\n",
    "        fontsize=20,           # é¡¹ç›®å­—ä½“å¤§å°\n",
    "        frameon=False,          # æ˜¾ç¤ºè¾¹æ¡†\n",
    "        framealpha=1,        # è¾¹æ¡†é€æ˜åº¦\n",
    "        edgecolor='white',   # è¾¹æ¡†é¢œè‰²\n",
    "        facecolor='white'      # èƒŒæ™¯é¢œè‰²\n",
    "    )\n",
    "    \n",
    "    # ä¸»å›¾è¾¹æ¡†ï¼ˆé—­åˆï¼Œçº¿å®½3ï¼‰\n",
    "    for spine in g.ax_joint.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_edgecolor('#333333')\n",
    "        spine.set_linewidth(2)\n",
    "    \n",
    "    # è®¡ç®—å®é™…å€¼å’Œé¢„æµ‹å€¼çš„å…¨å±€èŒƒå›´\n",
    "    all_values = np.concatenate([y_train, y_test, pp_tr, y_predicted])\n",
    "    min_val = np.min(all_values)\n",
    "    max_val = np.max(all_values)\n",
    "    buffer = (max_val - min_val) * 0.15  # å¢åŠ ç¼“å†²åŒºï¼Œç¡®ä¿è¾¹ç¼˜æ•°æ®æœ‰è¶³å¤Ÿç©ºé—´\n",
    "    \n",
    "    # è®¾ç½®ä¸»å›¾åæ ‡èŒƒå›´\n",
    "    g.ax_joint.set_xlim(min_val - buffer, max_val + buffer)\n",
    "    g.ax_joint.set_ylim(min_val - buffer, max_val + buffer)\n",
    "    \n",
    "    # ç»˜åˆ¶ç†æƒ³é¢„æµ‹çº¿\n",
    "    g.ax_joint.plot([min_val, max_val], [min_val, max_val], '--', color='gray', alpha=0.5)\n",
    "    \n",
    "    # ä¼˜åŒ–è¾¹ç¼˜åˆ†å¸ƒçš„é¢ç§¯å›¾\n",
    "    # Xè½´é¢ç§¯å›¾ - ä½¿ç”¨æ›´åˆé€‚çš„å¸¦å®½å’ŒèŒƒå›´\n",
    "    sns.kdeplot(\n",
    "        x=y_train, ax=g.ax_marg_x, color='#DB7987', alpha=0.5,\n",
    "        fill=True, linewidth=0, label='Training', bw_adjust=1.5,\n",
    "        clip=(min_val - buffer, max_val + buffer)\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        x=y_test, ax=g.ax_marg_x, color='#82BBF0', alpha=0.5,\n",
    "        fill=True, linewidth=0, label='Testing', bw_adjust=1.5,\n",
    "        clip=(min_val - buffer, max_val + buffer)\n",
    "    )\n",
    "    \n",
    "    # yè½´é¢ç§¯å›¾ - ä½¿ç”¨æ›´åˆé€‚çš„å¸¦å®½å’ŒèŒƒå›´\n",
    "    sns.kdeplot(\n",
    "        y=pp_tr, ax=g.ax_marg_y, color='#DB7987', alpha=0.5,\n",
    "        fill=True, linewidth=0, label='Training', bw_adjust=1.5,\n",
    "        clip=(min_val - buffer, max_val + buffer)\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        y=y_predicted, ax=g.ax_marg_y, color='#82BBF0', alpha=0.5,\n",
    "        fill=True, linewidth=0, label='Testing', bw_adjust=1.5,\n",
    "        clip=(min_val - buffer, max_val + buffer)\n",
    "    )\n",
    "    \n",
    "    # éšè—è¾¹ç¼˜åˆ†å¸ƒçš„å›¾ä¾‹\n",
    "    legend_x = g.ax_marg_x.legend()\n",
    "    legend_x.set_visible(False)\n",
    "    legend_y = g.ax_marg_y.legend()\n",
    "    legend_y.set_visible(False)\n",
    "    \n",
    "    # è°ƒæ•´è¾¹ç¼˜åˆ†å¸ƒåæ ‡è½´ç²—ç»†å’Œåˆ»åº¦æ–¹å‘\n",
    "    for ax in [g.ax_marg_x, g.ax_marg_y]:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(2)\n",
    "    \n",
    "    # æ–°å¢ï¼šè®¾ç½®è¾¹ç¼˜åæ ‡è½´åˆ»åº¦çº¿æœå¤–\n",
    "        ax.tick_params(\n",
    "           axis='both', \n",
    "           which='major',\n",
    "           direction='out',  # ä½¿åˆ»åº¦çº¿æœå¤–\n",
    "           width=2,          # åˆ»åº¦çº¿ç²—ç»†ä¸è½´çº¿ä¸€è‡´\n",
    "           length=8          # åˆ»åº¦çº¿é•¿åº¦\n",
    "    )\n",
    "    \n",
    "    # ç¡®ä¿è¾¹ç¼˜åˆ†å¸ƒä¸ä¸»å›¾åæ ‡èŒƒå›´ä¸€è‡´\n",
    "    g.ax_marg_x.set_xlim(g.ax_joint.get_xlim())\n",
    "    g.ax_marg_y.set_ylim(g.ax_joint.get_ylim())\n",
    "    \n",
    "    # å…³é”®ä¿®æ”¹ï¼šåœ¨æ‰€æœ‰ç»˜å›¾æ“ä½œå®Œæˆåè®¾ç½®è½´æ ‡ç­¾\n",
    "    g.ax_joint.set_xlabel(\"Actual value\", fontsize=26, fontname='Arial', fontweight='bold')\n",
    "    g.ax_joint.set_ylabel(\"Predicted value\", fontsize=26, fontname='Arial', fontweight='bold')\n",
    "    \n",
    "    # è°ƒæ•´æ•´ä½“å¸ƒå±€ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´æ˜¾ç¤ºé¢ç§¯å›¾\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.12, right=0.95)\n",
    "    \n",
    "    # ä¿å­˜å›¾åƒ\n",
    "    plot_path = os.path.join(output_dir, \"actual_vs_predicted.png\")\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return plot_path\n",
    "\n",
    "# ==================== ä¸»ç¨‹åº ====================\n",
    "def main():\n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. æ•°æ®å‡†å¤‡\n",
    "    X, y = prepare_data(input_path, target_column)\n",
    "    objective, problem_type = determine_problem_type(y)\n",
    "    params = get_model_params(objective, random_state)\n",
    "    \n",
    "    # 2. å¤–å±‚æ•°æ®åˆ’åˆ†\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=outer_test_size, random_state=random_state)\n",
    "    \n",
    "    # 3. å¤–å±‚æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°\n",
    "    print(\"å¤–å±‚æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°...\")\n",
    "    outer_model, outer_metrics = train_and_evaluate(\n",
    "        X_train, y_train, X_test, y_test, params, problem_type)\n",
    "    joblib.dump(outer_model, model_path)\n",
    "    print(f\"\\nå¤–å±‚æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{model_path}\")\n",
    "    \n",
    "    # æ‰“å°å¤–å±‚è¯„ä¼°ç»“æœ\n",
    "    print(\"\\nå¤–å±‚æ¨¡å‹æ€§èƒ½ï¼š\")\n",
    "    print(pd.DataFrame([outer_metrics], index=['Value']))\n",
    "    \n",
    "    # 4. å†…å±‚5æŠ˜äº¤å‰éªŒè¯\n",
    "    print(f\"\\nå¼€å§‹å†…å±‚{n_splits}æŠ˜äº¤å‰éªŒè¯...\")\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    cv_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train), 1):\n",
    "        print(f\"\\næ­£åœ¨å¤„ç†ç¬¬ {fold} æŠ˜...\")\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model, metrics = train_and_evaluate(X_tr, y_tr, X_val, y_val, params, problem_type)\n",
    "        cv_metrics.append(metrics)\n",
    "        print(f\"ç¬¬ {fold} æŠ˜å®Œæˆ - éªŒè¯é›†R2: {metrics.get('Test R2', metrics.get('Test Accuracy')):.4f}\")\n",
    "    \n",
    "    # 5. äº¤å‰éªŒè¯ç»“æœåˆ†æ\n",
    "    cv_results = pd.DataFrame(cv_metrics)\n",
    "    print(\"\\näº¤å‰éªŒè¯ç»“æœæ±‡æ€»ï¼š\")\n",
    "    print(cv_results)\n",
    "    \n",
    "    mean_metrics = cv_results.mean().to_dict()\n",
    "    std_metrics = cv_results.std().to_dict()\n",
    "    \n",
    "    print(\"\\näº¤å‰éªŒè¯å¹³å‡æ€§èƒ½ï¼š\")\n",
    "    for metric in mean_metrics:\n",
    "        print(f\"{metric}: {mean_metrics[metric]:.4f} Â± {std_metrics[metric]:.4f}\")\n",
    "    \n",
    "    # 6. å¯è§†åŒ–ä¸ç»“æœä¿å­˜\n",
    "    plot_feature_importance(outer_model, output_dir)\n",
    "    \n",
    "    if problem_type == \"regression\":\n",
    "        plot_path = plot_actual_vs_predicted(X_train, y_train, X_test, y_test, \n",
    "                                           outer_model, output_dir)\n",
    "        print(f\"\\nå®é™…å€¼ vs é¢„æµ‹å€¼å›¾å·²ä¿å­˜è‡³ï¼š{plot_path}\")\n",
    "    \n",
    "    # 7. ä¿å­˜è¯„ä¼°ç»“æœ\n",
    "    results = {\n",
    "        'outer_test_r2': outer_metrics.get('Test R2', outer_metrics.get('Test Accuracy')),\n",
    "        'cv_mean_r2': mean_metrics.get('Test R2', mean_metrics.get('Test Accuracy')),\n",
    "        'cv_std_r2': std_metrics.get('Test R2', std_metrics.get('Test Accuracy'))\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"results_summary.txt\"), 'w') as f:\n",
    "        f.write(\"æ¨¡å‹è¯„ä¼°ç»“æœæ±‡æ€»:\\n\")\n",
    "        f.write(f\"å¤–å±‚æµ‹è¯•é›†R2: {results['outer_test_r2']:.4f}\\n\")\n",
    "        f.write(f\"äº¤å‰éªŒè¯å¹³å‡R2: {results['cv_mean_r2']:.4f} Â± {results['cv_std_r2']:.4f}\\n\")\n",
    "    \n",
    "    print(\"\\nè¯„ä¼°ç»“æœå·²ä¿å­˜è‡³ results_summary.txt\")\n",
    "    print(\"\\nåµŒå¥—äº¤å‰éªŒè¯å®Œæˆï¼æ‰€æœ‰ç»“æœä¿å­˜åœ¨ç›®å½•ï¼š\", output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beb0f7dd-9bc1-4cee-a97c-6552f7560e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and model...\n",
      "Preparing data for SHAP analysis...\n",
      "Calculating SHAP values...\n",
      "Generating SHAP plots...\n",
      "Generating Waterfall plots...\n",
      "Successfully generated waterfall plot for sample 0\n",
      "Successfully generated waterfall plot for sample 5\n",
      "Successfully generated waterfall plot for sample 233\n",
      "\n",
      "SHAP analysis completed!\n",
      "Results saved to: C:\\Users\\miaoxinyi\\NH3-SCR with SO2\\shap_analysis_p\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# ==================== å‚æ•°é…ç½® ====================\n",
    "model_path = \"xgboost_model_p/xgboost_model.pkl\"  # æ¨¡å‹è·¯å¾„\n",
    "input_path = \"é¢„æµ‹æ•°æ®å½’ä¸€åŒ–å-20251120.xlsx\"                # è¾“å…¥æ•°æ®è·¯å¾„\n",
    "output_dir = \"shap_analysis_p\"                     # è¾“å‡ºç›®å½•\n",
    "target_column = \"SO2 tolerance\"                   # ç›®æ ‡å˜é‡åˆ—å\n",
    "random_state = 42                                 # éšæœºç§å­\n",
    "\n",
    "# è®¾ç½®è‹±æ–‡å­—ä½“\n",
    "plt.rcParams[\"font.family\"] = [\"Arial\"]\n",
    "plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n",
    "\n",
    "\n",
    "# ==================== å‡½æ•°å®šä¹‰ ====================\n",
    "def load_data_and_model():\n",
    "    \"\"\"åŠ è½½æ•°æ®å’Œæ¨¡å‹\"\"\"\n",
    "    model = joblib.load(model_path)\n",
    "    df = pd.read_excel(input_path)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    return X[numeric_cols], y, model\n",
    "\n",
    "def prepare_shap_data(X, y):\n",
    "    \"\"\"å‡†å¤‡SHAPåˆ†ææ•°æ®\"\"\"\n",
    "    return X.sample(n=min(235, len(X)), random_state=random_state)\n",
    "\n",
    "def plot_shap_summary(shap_values, X, output_dir):\n",
    "    \"\"\"ç»˜åˆ¶SHAPæ‘˜è¦å›¾\"\"\"\n",
    "    plt.figure(figsize=(14, 10), facecolor='white')\n",
    "    \n",
    "    # è®¡ç®—åˆç†çš„xè½´èŒƒå›´\n",
    "    max_shap = max(abs(shap_values.min()), abs(shap_values.max()))\n",
    "    xlim = (-max_shap * 1.1, max_shap * 1.1)  # ç¨å¾®æ‰©å¤§èŒƒå›´\n",
    "    \n",
    "    # é€‚é…æ–°ç‰ˆSHAP\n",
    "    if isinstance(shap_values, list):\n",
    "        shap.summary_plot(shap_values[0], X, show=False, plot_size=None)\n",
    "    else:\n",
    "        shap.summary_plot(shap_values, X, show=False, plot_size=None)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.grid(False)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.set_xlim(xlim)  # è®¾ç½®å¯¹ç§°çš„xè½´èŒƒå›´\n",
    "    \n",
    "    # è®¾ç½®æ ‡é¢˜æ ·å¼\n",
    "    plt.title(\"SHAP Value Distribution by Feature Importance\",\n",
    "              fontsize=26,\n",
    "              pad=20,\n",
    "              fontname='Arial',\n",
    "              fontweight='bold')\n",
    "\n",
    "    # ===== åæ ‡è½´æ ‡ç­¾è®¾ç½® =====\n",
    "    ax.set_xlabel(\"SHAP Value\",\n",
    "                  fontsize=26,\n",
    "                  fontname='Arial',\n",
    "                  fontweight='bold',\n",
    "                  labelpad=10)\n",
    "\n",
    "    ax.set_ylabel(\"Feature\",\n",
    "                  fontsize=22,\n",
    "                  fontname='Arial',\n",
    "                  fontweight='bold',\n",
    "                  labelpad=10)\n",
    "\n",
    "    # ===== åˆ»åº¦æ ‡ç­¾è®¾ç½® =====\n",
    "    ax.tick_params(axis='x',\n",
    "                   which='both',\n",
    "                   labelsize=22,\n",
    "                   width=1.5,\n",
    "                   labelfontfamily='Arial',\n",
    "                   length=6,\n",
    "                   direction='out')\n",
    "\n",
    "    ax.tick_params(axis='y',\n",
    "                   which='both',\n",
    "                   labelsize=22,\n",
    "                   width=1.5,\n",
    "                   labelfontfamily='Arial',\n",
    "                   length=6,\n",
    "                   direction='out')\n",
    "\n",
    "    # è®¾ç½®åæ ‡è½´æ ·å¼\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(3)\n",
    "    \n",
    "    # ä¿å­˜å›¾å½¢\n",
    "    plot_path = os.path.join(output_dir, \"shap_summary.png\")\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close()\n",
    "    return plot_path\n",
    "\n",
    "def plot_shap_bar(shap_values, X, output_dir):\n",
    "    \"\"\"ç»˜åˆ¶SHAPæ¡å½¢å›¾\"\"\"\n",
    "    plt.figure(figsize=(12, 8), facecolor='white')\n",
    "    \n",
    "    # é€‚é…æ–°ç‰ˆSHAP\n",
    "    if isinstance(shap_values, list):\n",
    "        shap.summary_plot(shap_values[0], X, plot_type=\"bar\", show=False)\n",
    "    else:\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.grid(False)\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    # è°ƒæ•´è¾¹è·\n",
    "    plt.subplots_adjust(left=0.3, right=0.95)\n",
    "    \n",
    "    # è®¾ç½®åæ ‡è½´æ ·å¼\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(3)\n",
    "    \n",
    "    plot_path = os.path.join(output_dir, \"shap_bar.png\")\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close()\n",
    "    return plot_path\n",
    "\n",
    "def plot_shap_dependence(shap_values, X, output_dir):\n",
    "    \"\"\"ç»˜åˆ¶SHAPä¾èµ–å›¾\"\"\"\n",
    "    os.makedirs(os.path.join(output_dir, \"dependence_plots\"), exist_ok=True)\n",
    "    plot_paths = []\n",
    "    \n",
    "    # é€‚é…æ–°ç‰ˆSHAP\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_vals = shap_values[0]\n",
    "    else:\n",
    "        shap_vals = shap_values\n",
    "    \n",
    "    for feature in X.columns:\n",
    "        plt.figure(figsize=(10, 6), facecolor='white')\n",
    "        shap.dependence_plot(feature, shap_vals, X, show=False)\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        ax.grid(False)\n",
    "        ax.set_facecolor('none')\n",
    "        # è®¾ç½®åæ ‡è½´æ ·å¼\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(True)\n",
    "        ax.spines['bottom'].set_color('black')\n",
    "        ax.spines['bottom'].set_linewidth(3)\n",
    "        \n",
    "        plot_path = os.path.join(output_dir, \"dependence_plots\", f\"shap_dependence_{feature}.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight', transparent=True)\n",
    "        plt.close()\n",
    "        plot_paths.append(plot_path)\n",
    "        \n",
    "    return plot_paths\n",
    "def plot_shap_waterfall(explainer, shap_values, X_sample, sample_index, output_dir):\n",
    "    \"\"\"ç»˜åˆ¶SHAPç€‘å¸ƒå›¾\"\"\"\n",
    "    os.makedirs(os.path.join(output_dir, \"waterfall_plots\"), exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 10), facecolor='white')\n",
    "    \n",
    "    expected_value = explainer.expected_value\n",
    "    if isinstance(expected_value, list):\n",
    "        expected_value = expected_value[0]\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_vals = shap_values[0]\n",
    "    else:\n",
    "        shap_vals = shap_values\n",
    "    \n",
    "    # æ£€æŸ¥æ ·æœ¬ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ\n",
    "    if sample_index >= len(X_sample):\n",
    "        print(f\"Warning: Sample index {sample_index} out of range, using last sample\")\n",
    "        sample_index = len(X_sample) - 1\n",
    "    \n",
    "    # ç”Ÿæˆç€‘å¸ƒå›¾\n",
    "    shap.plots.waterfall(\n",
    "        shap.Explanation(\n",
    "            values=shap_vals[sample_index],\n",
    "            base_values=expected_value,\n",
    "            data=X_sample.iloc[sample_index],\n",
    "            feature_names=X_sample.columns.tolist()\n",
    "        ),\n",
    "        show=False,\n",
    "        max_display=10  # é™åˆ¶æ˜¾ç¤ºçš„ç‰¹å¾æ•°é‡\n",
    "    )\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.grid(False)\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # è®¾ç½®å­—ä½“æ ·å¼\n",
    "    font_props = {\n",
    "        'family': 'Arial',  # å­—ä½“ç±»å‹\n",
    "        'weight': 'bold',   # å­—ä½“åŠ ç²—\n",
    "        'size': 14          # å­—ä½“å¤§å°\n",
    "    }\n",
    "    \n",
    "    # è®¾ç½®åæ ‡è½´æ ‡ç­¾æ ·å¼\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontdict=font_props)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontdict=font_props)\n",
    "    \n",
    "    # è®¾ç½®åˆ»åº¦æ ‡ç­¾æ ·å¼\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "        label.set_fontname('Arial')\n",
    "    for label in ax.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "        label.set_fontname('Arial')\n",
    "    \n",
    "    # è®¾ç½®æ ‡é¢˜æ ·å¼ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "    if ax.get_title():\n",
    "        ax.set_title(ax.get_title(), fontdict={\n",
    "            'fontsize': 14,\n",
    "            'fontweight': 'bold',\n",
    "            'fontname': 'Arial'\n",
    "        })\n",
    "    \n",
    "    # è®¾ç½®å…¶ä»–æ–‡æœ¬å…ƒç´ çš„æ ·å¼ï¼ˆå¦‚ç‰¹å¾åç§°ç­‰ï¼‰\n",
    "    for text in ax.texts:\n",
    "        text.set_fontweight('bold')\n",
    "        text.set_fontname('Arial')\n",
    "        text.set_fontsize(14)\n",
    "\n",
    "    # è°ƒæ•´è¾¹è·\n",
    "    plt.subplots_adjust(left=0.4, right=0.9)\n",
    "\n",
    "    # è®¾ç½®åæ ‡è½´æ ·å¼\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['top'].set_linewidth(2)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['right'].set_color('black')\n",
    "    ax.spines['right'].set_linewidth(2)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(3.5)\n",
    "\n",
    "    plot_path = os.path.join(output_dir, \"waterfall_plots\", f\"shap_waterfall_{sample_index}.png\")\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close()\n",
    "    return plot_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ä½¿ç”¨å…¨å±€å®šä¹‰çš„è¾“å‡ºç›®å½•ï¼Œè€Œä¸æ˜¯é‡æ–°å®šä¹‰\n",
    "    global output_dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Loading data and model...\")\n",
    "    X, y, model = load_data_and_model()\n",
    "\n",
    "    print(\"Preparing data for SHAP analysis...\")\n",
    "    X_shap = prepare_shap_data(X, y)\n",
    "\n",
    "    print(\"Calculating SHAP values...\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    # é€‚é…æ–°ç‰ˆSHAP API\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "    if not isinstance(shap_values, list):\n",
    "        # ç¡®ä¿shap_valuesæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå…¼å®¹åç»­ä»£ç \n",
    "        shap_values = [shap_values]\n",
    "\n",
    "    print(\"Generating SHAP plots...\")\n",
    "    summary_path = plot_shap_summary(shap_values[0], X_shap, output_dir)\n",
    "    bar_path = plot_shap_bar(shap_values[0], X_shap, output_dir)\n",
    "    dependence_paths = plot_shap_dependence(shap_values, X_shap, output_dir)\n",
    "\n",
    "    print(\"Generating Waterfall plots...\")\n",
    "    waterfall_paths = []\n",
    "    sample_indices = [0, 5, min(233, len(X_shap)-1)]  # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ\n",
    "    for i in sample_indices:\n",
    "        try:\n",
    "            path = plot_shap_waterfall(explainer, shap_values, X_shap, i, output_dir)\n",
    "            waterfall_paths.append(path)\n",
    "            print(f\"Successfully generated waterfall plot for sample {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating waterfall plot for sample {i}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()  # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆä¿¡æ¯\n",
    "\n",
    "    # ä¿å­˜SHAPå€¼\n",
    "    shap_df = pd.DataFrame(shap_values[0], columns=X_shap.columns)\n",
    "    shap_df.to_csv(os.path.join(output_dir, \"shap_values.csv\"), index=False)\n",
    "    \n",
    "    # ä¿å­˜ç‰¹å¾é‡è¦æ€§æ’åº\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_shap.columns,\n",
    "        'Mean SHAP': abs(shap_df).mean().values\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('Mean SHAP', ascending=False)\n",
    "    importance_df.to_csv(os.path.join(output_dir, \"feature_importance.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nSHAP analysis completed!\")\n",
    "    print(f\"Results saved to: {os.path.abspath(output_dir)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b4bf84-159c-47a7-b465-b733e47ef762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== SHAP Force Plot åˆ†æå¼€å§‹ ======\n",
      "\n",
      "ğŸ” åŠ è½½æ•°æ®å’Œæ¨¡å‹...\n",
      "\n",
      "ğŸ“Š æ­£åœ¨ç”ŸæˆForce Plot...\n",
      "âœ… æ ·æœ¬ 230 åˆ†æå®Œæˆ -> shap_force_analysis_P\\force_plot_sample_230.png\n",
      "âœ… æ ·æœ¬ 231 åˆ†æå®Œæˆ -> shap_force_analysis_P\\force_plot_sample_231.png\n",
      "âœ… æ ·æœ¬ 232 åˆ†æå®Œæˆ -> shap_force_analysis_P\\force_plot_sample_232.png\n",
      "âœ… æ ·æœ¬ 233 åˆ†æå®Œæˆ -> shap_force_analysis_P\\force_plot_sample_233.png\n",
      "âœ… æ ·æœ¬ 234 åˆ†æå®Œæˆ -> shap_force_analysis_P\\force_plot_sample_234.png\n",
      "âœ… æ ·æœ¬ 235 åˆ†æå®Œæˆ -> shap_force_analysis_P\\force_plot_sample_235.png\n",
      "\n",
      "ğŸ‰ åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: C:\\Users\\miaoxinyi\\NH3-SCR with SO2\\shap_force_analysis_P\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ä¼˜åŒ–ç‰ˆSHAP Force Plotè„šæœ¬\n",
    "åŠŸèƒ½ï¼šç”Ÿæˆæ¸…æ™°çš„ä¸ªä½“é¢„æµ‹è§£é‡Šå›¾ï¼Œæ ‡æ³¨åŸºå‡†å€¼å’Œé¢„æµ‹å€¼\n",
    "\"\"\"\n",
    "\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# ==================== å‚æ•°é…ç½® ====================\n",
    "MODEL_PATH = \"xgboost_model_P/xgboost_model.pkl\"  # æ¨¡å‹è·¯å¾„\n",
    "DATA_PATH = \"é¢„æµ‹æ•°æ®å½’ä¸€åŒ–å-20251120.xlsx\"                      # æ•°æ®è·¯å¾„\n",
    "OUTPUT_DIR = \"shap_force_analysis_P\"                # è¾“å‡ºç›®å½•\n",
    "TARGET_COL = \"SO2 tolerance\"                     # ç›®æ ‡åˆ—å\n",
    "SAMPLE_INDICES = [230, 231, 232, 233, 234, 235]                    # æŒ‡å®šåˆ†ææ ·æœ¬ç´¢å¼•\n",
    "DPI = 300                                        # è¾“å‡ºåˆ†è¾¨ç‡\n",
    "\n",
    "# å¯è§†åŒ–å‚æ•°\n",
    "FONT_CONFIG = {\n",
    "    'family': 'Times New Roman',\n",
    "    'size': 22\n",
    "}\n",
    "\n",
    "COLOR_PALETTE = {\n",
    "    'positive': '#ff0051',  # æ­£å‘å½±å“(çº¢è‰²)\n",
    "    'negative': '#008bfb',  # è´Ÿå‘å½±å“(è“è‰²)\n",
    "    'baseline': '#333333'   # åŸºå‡†çº¿é¢œè‰²\n",
    "}\n",
    "\n",
    "# ==================== ä¸»å‡½æ•° ====================\n",
    "def main():\n",
    "    print(\"====== SHAP Force Plot åˆ†æå¼€å§‹ ======\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # è®¾ç½®å…¨å±€æ ·å¼\n",
    "    rcParams.update({\n",
    "        'font.family': FONT_CONFIG['family'],\n",
    "        'font.size': FONT_CONFIG['size'],\n",
    "        'figure.facecolor': 'white'\n",
    "    })\n",
    "\n",
    "    # åŠ è½½æ•°æ®å’Œæ¨¡å‹\n",
    "    print(\"\\nğŸ” åŠ è½½æ•°æ®å’Œæ¨¡å‹...\")\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "    X = df.drop(columns=[TARGET_COL]).select_dtypes(include=['number'])\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # åˆå§‹åŒ–è§£é‡Šå™¨\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    print(\"\\nğŸ“Š æ­£åœ¨ç”ŸæˆForce Plot...\")\n",
    "    for idx in SAMPLE_INDICES:\n",
    "        try:\n",
    "            sample = X.iloc[[idx]]\n",
    "            shap_values = explainer.shap_values(sample)\n",
    "            \n",
    "            # è®¡ç®—é¢„æµ‹å€¼\n",
    "            base_value = explainer.expected_value\n",
    "            predicted_value = base_value + np.sum(shap_values[0])\n",
    "            \n",
    "            # åˆ›å»ºå›¾å½¢\n",
    "            plt.figure(figsize=(12, 8), facecolor='white')\n",
    "            ax = plt.gca()\n",
    "            \n",
    "            # ç»˜åˆ¶force plot\n",
    "            force_plot = shap.force_plot(\n",
    "                base_value=base_value,\n",
    "                shap_values=shap_values,\n",
    "                features=sample,\n",
    "                feature_names=feature_names,\n",
    "                show=False,\n",
    "                matplotlib=True,\n",
    "                plot_cmap=[COLOR_PALETTE['positive'], COLOR_PALETTE['negative']],\n",
    "                text_rotation=45  # ç‰¹å¾æ ‡æ³¨æ—‹è½¬45åº¦\n",
    "            )\n",
    "            \n",
    "            # æ ‡æ³¨åŸºå‡†å€¼å’Œé¢„æµ‹å€¼\n",
    "            plt.axvline(x=base_value, color=COLOR_PALETTE['baseline'], linestyle='--', \n",
    "                        label=f'Base Value: {base_value:.3f}')\n",
    "            plt.axvline(x=predicted_value, color='green', linestyle=':', \n",
    "                       label=f'Predicted Value: {predicted_value:.3f}')\n",
    "            \n",
    "            # æ·»åŠ å›¾ä¾‹åˆ°å³ä¾§\n",
    "            plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            \n",
    "            # è°ƒæ•´å¸ƒå±€\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # ä¿å­˜å›¾å½¢\n",
    "            output_path = os.path.join(OUTPUT_DIR, f\"force_plot_sample_{idx}.png\")\n",
    "            plt.savefig(output_path, dpi=DPI, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"âœ… æ ·æœ¬ {idx} åˆ†æå®Œæˆ -> {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ ·æœ¬ {idx} åˆ†æå¤±è´¥: {str(e)}\")\n",
    "\n",
    "    print(\"\\nğŸ‰ åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨:\", os.path.abspath(OUTPUT_DIR))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e47bdd6-ec54-419d-a749-36ce22691293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½æ¨¡å‹: xgboost_model_p/xgboost_model.pkl\n",
      "æ¨¡å‹åŠ è½½æˆåŠŸ\n",
      "\n",
      "æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾ (19 ä¸ª):\n",
      "  EN., I.E., I.R., M.P., T.C., Density, C.Tem., C.Tim., S.A., P.V., P.D., SO2, NH3, NO, O2, H2O, GHSV , P.Tim., P.Tem.\n",
      "\n",
      "ä»è®­ç»ƒæ•°æ®è·å–å¡«å……ç”¨çš„ä¸­ä½æ•°...\n",
      "å¡«å……ç­–ç•¥è·å–æˆåŠŸ\n",
      "åŸå§‹æ–°æ•°æ®å½¢çŠ¶: (248, 19)\n",
      "æœªæ£€æµ‹åˆ°ç¼ºå¤±å€¼\n",
      "\n",
      "é¢„å¤„ç†å®Œæˆï¼Œç‰¹å¾çŸ©é˜µå½¢çŠ¶: (248, 19)\n",
      "\n",
      "æ­£åœ¨é¢„æµ‹...\n",
      "é¢„æµ‹å®Œæˆ\n",
      "\n",
      "ä¿å­˜ç»“æœè‡³: SO2 tolerance prediction_results.xlsx\n",
      "ç»“æœä¿å­˜æˆåŠŸï¼\n",
      "\n",
      "--- é¢„æµ‹ç»“æœé¢„è§ˆ ---\n",
      "        EN.      I.E.      I.R.      M.P.      T.C.  Predicted_SO2 tolerance\n",
      "0  0.767471  0.945662  0.615082  0.551861  0.972925                 0.272027\n",
      "1  0.767471  0.945662  0.615082  0.551861  0.976339                 0.256551\n",
      "2  0.754660  0.948052  0.576412  0.549170  0.997813                 0.334440\n",
      "3  0.754660  0.948052  0.576412  0.549170  0.997813                 0.314221\n",
      "4  0.754660  0.948052  0.576412  0.549170  0.997813                 0.303096\n",
      "5  0.774003  0.946012  0.581822  0.551676  0.998050                 0.297058\n",
      "6  0.774003  0.946012  0.581822  0.551676  0.998050                 0.272533\n",
      "7  0.774003  0.946012  0.581822  0.551676  0.998050                 0.251787\n",
      "8  0.727908  0.944578  0.582319  0.547530  0.970092                 0.198642\n",
      "9  0.727908  0.944578  0.582319  0.547530  0.970092                 0.190402\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import xgboost as xgb\n",
    "\n",
    "# ==================== å‚æ•°é…ç½® ====================\n",
    "model_path = \"xgboost_model_p/xgboost_model.pkl\"  # è®­ç»ƒå¥½çš„Boosteræ¨¡å‹\n",
    "new_data_path = \"å½’ä¸€åŒ–æ•°æ®-20251120-SO2 tolerance prediciton.xlsx\"                     # å¾…é¢„æµ‹æ•°æ®\n",
    "output_path = \"SO2 tolerance prediction_results.xlsx\"             # é¢„æµ‹ç»“æœè¾“å‡º\n",
    "target_column = \"SO2 tolerance\"                     # ç›®æ ‡å˜é‡åï¼ˆæ–°æ•°æ®ä¸­å¯ä¸å­˜åœ¨ï¼‰\n",
    "\n",
    "# ==================== å‡½æ•°å®šä¹‰ ====================\n",
    "def load_and_prepare_new_data(new_data_path, model_features, train_median):\n",
    "    \"\"\"\n",
    "    åŠ è½½æ–°æ•°æ®ï¼Œå¤„ç†ç¼ºå¤±å€¼ï¼Œå¹¶ç¡®ä¿ç‰¹å¾ä¸æ¨¡å‹ä¸€è‡´\n",
    "    \"\"\"\n",
    "    # 1. åŠ è½½æ–°æ•°æ®\n",
    "    if new_data_path.endswith('.csv'):\n",
    "        df_new = pd.read_csv(new_data_path)\n",
    "    elif new_data_path.endswith('.xlsx'):\n",
    "        df_new = pd.read_excel(new_data_path)\n",
    "    else:\n",
    "        raise ValueError(\"ä»…æ”¯æŒ .csv æˆ– .xlsx æ ¼å¼\")\n",
    "\n",
    "    print(f\"åŸå§‹æ–°æ•°æ®å½¢çŠ¶: {df_new.shape}\")\n",
    "\n",
    "    # 2. å¤„ç†ç¼ºå¤±å€¼ï¼ˆç”¨è®­ç»ƒé›†çš„ä¸­ä½æ•°å¡«å……ï¼‰\n",
    "    missing_mask = df_new.isnull()\n",
    "    if missing_mask.any().any():\n",
    "        print(\"\\næ£€æµ‹åˆ°ç¼ºå¤±å€¼ï¼Œç”¨è®­ç»ƒé›†ä¸­ä½æ•°å¡«å……ï¼š\")\n",
    "        df_filled = df_new.fillna(train_median)\n",
    "        # æ‰“å°å¡«å……ä¿¡æ¯\n",
    "        filled_cols = missing_mask.sum()[missing_mask.sum() > 0].index\n",
    "        for col in filled_cols:\n",
    "            count = missing_mask[col].sum()\n",
    "            print(f\"  - {col}: {count} ä¸ªç¼ºå¤±å€¼ï¼Œå¡«å……å€¼ = {train_median[col]:.4f}\")\n",
    "    else:\n",
    "        print(\"æœªæ£€æµ‹åˆ°ç¼ºå¤±å€¼\")\n",
    "        df_filled = df_new.copy()\n",
    "\n",
    "    # 3. ç¡®ä¿ç‰¹å¾ä¸æ¨¡å‹è®­ç»ƒæ—¶ä¸€è‡´\n",
    "    missing_features = set(model_features) - set(df_filled.columns)\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"æ–°æ•°æ®ç¼ºå°‘å¿…éœ€ç‰¹å¾: {', '.join(missing_features)}\")\n",
    "    \n",
    "    # æŒ‰æ¨¡å‹ç‰¹å¾é¡ºåºæ’åˆ—\n",
    "    X_processed = df_filled[model_features].copy()\n",
    "    print(f\"\\né¢„å¤„ç†å®Œæˆï¼Œç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_processed.shape}\")\n",
    "    return df_new, X_processed\n",
    "\n",
    "def main():\n",
    "    # 1. åŠ è½½è®­ç»ƒå¥½çš„Boosteræ¨¡å‹\n",
    "    print(f\"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ '{model_path}' ä¸å­˜åœ¨\")\n",
    "        return\n",
    "    model = joblib.load(model_path)\n",
    "    print(\"æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "\n",
    "    # 2. è·å–æ¨¡å‹è®­ç»ƒæ—¶çš„ç‰¹å¾åï¼ˆå…³é”®ä¿®å¤ï¼šç›´æ¥è®¿é—® feature_namesï¼‰\n",
    "    model_features = model.feature_names\n",
    "    print(f\"\\næ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾ ({len(model_features)} ä¸ª):\")\n",
    "    print(f\"  {', '.join(model_features)}\")\n",
    "\n",
    "    # 3. åŠ è½½è®­ç»ƒé›†æ•°æ®ï¼Œè·å–ç¼ºå¤±å€¼å¡«å……ç”¨çš„ä¸­ä½æ•°ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰\n",
    "    train_data_path = \"å½’ä¸€åŒ–æ•°æ®-20251120.xlsx\"\n",
    "    if not os.path.exists(train_data_path):\n",
    "        print(f\"é”™è¯¯: è®­ç»ƒæ•°æ® '{train_data_path}' ä¸å­˜åœ¨ï¼Œæ— æ³•è·å–å¡«å……ç­–ç•¥\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nä»è®­ç»ƒæ•°æ®è·å–å¡«å……ç”¨çš„ä¸­ä½æ•°...\")\n",
    "    df_train = pd.read_excel(train_data_path)\n",
    "    # åªä¿ç•™æ¨¡å‹ä½¿ç”¨çš„ç‰¹å¾ï¼Œè®¡ç®—ä¸­ä½æ•°\n",
    "    train_features_df = df_train[model_features]\n",
    "    train_median = train_features_df.median().to_dict()\n",
    "    print(\"å¡«å……ç­–ç•¥è·å–æˆåŠŸ\")\n",
    "\n",
    "    # 4. åŠ è½½å¹¶é¢„å¤„ç†æ–°æ•°æ®\n",
    "    try:\n",
    "        df_original_new, X_new_processed = load_and_prepare_new_data(\n",
    "            new_data_path, model_features, train_median\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\né¢„å¤„ç†å¤±è´¥: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 5. æ‰§è¡Œé¢„æµ‹ï¼ˆBoosterå¯¹è±¡éœ€è¦ç”¨DMatrixåŒ…è£…ï¼‰\n",
    "    print(\"\\næ­£åœ¨é¢„æµ‹...\")\n",
    "    dmatrix_new = xgb.DMatrix(X_new_processed)\n",
    "    predictions = model.predict(dmatrix_new)\n",
    "    print(\"é¢„æµ‹å®Œæˆ\")\n",
    "\n",
    "    # 6. ä¿å­˜ç»“æœ\n",
    "    print(f\"\\nä¿å­˜ç»“æœè‡³: {output_path}\")\n",
    "    df_results = df_original_new.copy()\n",
    "    df_results[f\"Predicted_{target_column}\"] = predictions\n",
    "    df_results.to_excel(output_path, index=False)\n",
    "    print(\"ç»“æœä¿å­˜æˆåŠŸï¼\")\n",
    "\n",
    "    # 7. é¢„è§ˆç»“æœ\n",
    "    print(\"\\n--- é¢„æµ‹ç»“æœé¢„è§ˆ ---\")\n",
    "    preview_cols = list(df_results.columns[:min(5, len(df_results.columns)-1)]) + [f\"Predicted_{target_column}\"]\n",
    "    print(df_results[preview_cols].head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceee28c-d0bb-422b-944b-2c11862c7744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
